{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import cPickle\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordPOSTagger('../Tools/stanford-postagger-full-2017-06-09/models/english-bidirectional-distsim.tagger','../Tools/stanford-postagger-full-2017-06-09/stanford-postagger-3.8.0.jar')\n",
    "sp = StanfordParser(path_to_jar='../Tools/stanford-parser-full-2017-06-09/stanford-parser.jar',path_to_models_jar='../Tools/stanford-parser-full-2017-06-09/stanford-parser-3.8.0-models.jar')\n",
    "sdp = StanfordDependencyParser(path_to_jar='../Tools/stanford-parser-full-2017-06-09/stanford-parser.jar',path_to_models_jar='../Tools/stanford-parser-full-2017-06-09/stanford-parser-3.8.0-models.jar')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strong_polar_words = []\n",
    "weak_polar_words = []\n",
    "positive_polar_words = []\n",
    "negative_polar_words = []\n",
    "with open('../Datasets/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.split()[0].split('=')[1] == 'weaksubj':\n",
    "            weak_polar_words.append(line.split()[2].split('=')[1])\n",
    "        elif line.split()[0].split('=')[1] == 'strongsubj':\n",
    "            strong_polar_words.append(line.split()[2].split('=')[1])\n",
    "        if line.split()[5].split('=')[1] == 'positive':\n",
    "            positive_polar_words.append(line.split()[2].split('=')[1])\n",
    "        elif line.split()[5].split('=')[1] == 'negative':\n",
    "            negative_polar_words.append(line.split()[2].split('=')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liwc_lexicons = []\n",
    "\n",
    "with open('../Datasets/LIWC2007dictionary.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.next()\n",
    "    reader.next()\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        ## Feel\n",
    "        if row[68] != '':\n",
    "            liwc_lexicons.append(row[68])\n",
    "        \n",
    "        ## Swear\n",
    "        if row[26] != '':\n",
    "            liwc_lexicons.append(row[26])\n",
    "            \n",
    "        ## Certain\n",
    "        if row[60] != '':\n",
    "            liwc_lexicons.append(row[60])\n",
    "            \n",
    "        ## Percept\n",
    "        if row[64] != '':\n",
    "            liwc_lexicons.append(row[64])\n",
    "            \n",
    "        if row[65] != '':\n",
    "            liwc_lexicons.append(row[65])\n",
    "            \n",
    "        ## Time\n",
    "        if row[87] != '':\n",
    "            liwc_lexicons.append(row[87])\n",
    "            \n",
    "        if row[88] != '':\n",
    "            liwc_lexicons.append(row[88])\n",
    "liwc_lexicons = [i if '*' not in i else i[:-1] for i in liwc_lexicons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"strong_polar_words\",\"weak_polar_words\",\"root_verb\",\"acomp\",\"xcomp\",\"advmod\",\"modals\",\"pronouns\",\"LIWC\"]\n",
    "\n",
    "def extract_features(text):\n",
    "    features = np.zeros(len(FEATURES))\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(w) for w in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    pos_tags = [i[1] for i in st.tag(words)]\n",
    "    dep_triplets = list(sdp.raw_parse(text).next().triples())\n",
    "    deps = [i[1] for i in dep_triplets]\n",
    "    root_verb = [parse.tree() for parse in sdp.raw_parse(text)][0].label()\n",
    "\n",
    "    ## strong and weak polar words\n",
    "    features[FEATURES.index(\"strong_polar_words\")] = 0\n",
    "    features[FEATURES.index(\"weak_polar_words\")] = 0\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in strong_polar_words or stemmed_words[i] in strong_polar_words or lemmatized_words[i] in strong_polar_words:\n",
    "            features[FEATURES.index(\"strong_polar_words\")] += 1\n",
    "        elif words[i] in weak_polar_words or stemmed_words[i] in weak_polar_words or lemmatized_words[i] in weak_polar_words:\n",
    "            features[FEATURES.index(\"weak_polar_words\")] += 1\n",
    "    \n",
    "    ## polarity of root verb\n",
    "    features[FEATURES.index(\"root_verb\")] = 1 if root_verb in positive_polar_words or stemmer.stem(root_verb) in positive_polar_words or lemmatizer.lemmatize(root_verb) in positive_polar_words else -1 if root_verb in negative_polar_words or stemmer.stem(root_verb) in negative_polar_words or lemmatizer.lemmatize(root_verb) in negative_polar_words else 0\n",
    "    \n",
    "    ## presence of aComp, xComp and advMod dependencies\n",
    "    features[FEATURES.index(\"acomp\")] = 1 if 'acomp' in deps else 0\n",
    "    features[FEATURES.index(\"xcomp\")] = 1 if 'xcomp' in deps else 0\n",
    "    features[FEATURES.index(\"advmod\")] = 1 if 'advmod' in deps else 0\n",
    "    \n",
    "    ## opionated n-grams\n",
    "    \n",
    "    \n",
    "    ## presence of modal verbs\n",
    "    features[FEATURES.index(\"modals\")] = 1 if 'MD' in pos_tags else 0\n",
    "    \n",
    "    ## presence of pronouns\n",
    "    features[FEATURES.index(\"pronouns\")] = 1 if \"PRP\" in pos_tags or \"PRP$\" in pos_tags or \"WP\" in pos_tags or \"WP$\" in pos_tags else 0\n",
    "    \n",
    "    ## opinionated words (?)    \n",
    "    ## LIWC features\n",
    "    features[FEATURES.index(\"LIWC\")] = len([i for i in range(len(words)) if words[i] in liwc_lexicons or stemmed_words[i] in liwc_lexicons or lemmatized_words[i] in liwc_lexicons])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5d50712ec1a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mmpqa_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mmpqa_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-393b7756bc06>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdep_triplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdep_triplets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mroot_verb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projjal/anaconda2/lib/python2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36mraw_parse\u001b[0;34m(self, sentence, verbose)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projjal/anaconda2/lib/python2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[0;34m(self, sentences, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;34m'-outputFormat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_OUTPUT_FORMAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         ]\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_trees_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtagged_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projjal/anaconda2/lib/python2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m_parse_trees_output\u001b[0;34m(self, output_)\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mblank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                     \u001b[0mcur_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projjal/anaconda2/lib/python2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m_make_tree\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDependencyGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_relation_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projjal/anaconda2/lib/python2.7/site-packages/nltk/parse/dependencygraph.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tree_str, cell_extractor, zero_based, cell_separator, top_relation_label)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mzero_based\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_based\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mcell_separator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_separator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mtop_relation_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_relation_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projjal/anaconda2/lib/python2.7/site-packages/nltk/parse/dependencygraph.pyc\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, input_, cell_extractor, zero_based, cell_separator, top_relation_label)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0mcell_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mcell_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcell_extractor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mpqa_texts = []\n",
    "mpqa_labels = []\n",
    "for root, dirs, files in os.walk('../Datasets/mpqa535/'):\n",
    "    for f in files:\n",
    "        with open(os.path.join(root,f)) as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            reader.next()\n",
    "            for row in reader:\n",
    "                mpqa_texts.append(extract_features(row[0]))\n",
    "                mpqa_labels.append(0 if row[1] == 'f' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yahoo_texts = []\n",
    "yahoo_labels = []\n",
    "for root, dirs, files in os.walk('../Datasets/120/'):\n",
    "    for f in files:\n",
    "        with open(os.path.join(root,f)) as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            reader.next()\n",
    "            for row in reader:\n",
    "                yahoo_texts.append(extract_features(row[0]))\n",
    "                yahoo_labels.append(0 if row[1] == 'f' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_mpqa_texts = np.asarray(mpqa_texts)\n",
    "np_mpqa_labels = np.asarray(mpqa_labels)\n",
    "\n",
    "np_yahoo_texts = np.asarray(yahoo_texts)\n",
    "np_yahoo_labels = np.asarray(yahoo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('np_mpqa_texts.npy',np_mpqa_texts)\n",
    "np.save('np_mpqa_labels.npy',np_mpqa_labels)\n",
    "np.save('np_yahoo_texts.npy',np_yahoo_texts)\n",
    "np.save('np_yahoo_labels.npy',np_yahoo_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
